{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear Regression with Regularization\n",
    "=====================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a way to prevent overfitting and allows the model to\n",
    "generalize better. We’ll cover the *Ridge* and *Lasso* regression here.\n",
    "\n",
    "The Need for Regularization\n",
    "---------------------------\n",
    "\n",
    "Unlike polynomial fitting, it’s hard to imagine how linear regression\n",
    "can overfit the data, since it’s just a single line (or a hyperplane).\n",
    "One situation is that features are **correlated** or redundant.\n",
    "\n",
    "Suppose there are two features, both are exactly the same, our predicted\n",
    "hyperplane will be in this format:\n",
    "\n",
    "\\begin{align}\\hat{y} = w_0 + w_1x_1 + w_2x_2\\end{align}\n",
    "\n",
    "and the true values of $x_2$ is almost the same as $x_1$ (or\n",
    "with some multiplicative factor and noise). Then, it’s best to just drop\n",
    "$w_2x_2$ term and use:\n",
    "\n",
    "\\begin{align}\\hat{y} = w_0 + w_1x_1\\end{align}\n",
    "\n",
    "to fit the data. This is a simpler model.\n",
    "\n",
    "But we don’t know whether $x_1$ and $x_2$ is **actually**\n",
    "redundant or not, at least with bare eyes, and we don’t want to manually\n",
    "drop a parameter just because we feel like it. We want to model to learn\n",
    "to do this itself, that is, to *prefer a simpler model that fits the\n",
    "data well enough*.\n",
    "\n",
    "To do this, we add a *penalty term* to our loss function. Two common\n",
    "penalty terms are L2 and L1 norm of $w$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 and L1 Penalty\n",
    "-----------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. No Penalty (or Linear)\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "This is linear regression without any regularization (from `previous\n",
    "article </blog_content/linear_regression/linear_regression_tutorial.html#writing-sse-loss-in-matrix-notation>`__):\n",
    "\n",
    "\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. L2 Penalty (or Ridge)\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "We can add the **L2 penalty term** to it, and this is called **L2\n",
    "regularization**.:\n",
    "\n",
    "\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2 + \\lambda\\sum_{j=0}^{d}w_j^2\\end{align}\n",
    "\n",
    "This is called L2 penalty just because it’s a L2-norm of $w$. In\n",
    "fancy term, this whole loss function is also known as **Ridge\n",
    "regression**.\n",
    "\n",
    "Let’s see what’s going on. Loss function is something we **minimize**.\n",
    "Any terms that we add to it, we also want it to be minimized (that’s why\n",
    "it’s called *penalty term*). The above means we want $w$ that fits\n",
    "the data well (first term), but we also want the values of $w$ to\n",
    "be small as possible (second term). The lambda ($\\lambda$) is\n",
    "there to adjust how much to penalize $w$. Note that ``sklearn``\n",
    "refers to this as alpha ($\\alpha$) instead, but whatever.\n",
    "\n",
    "It’s tricky to know the appropriate value for lambda. You just have to\n",
    "try them out, in exponential range (0.01, 0.1, 1, 10, etc), then select\n",
    "the one that has the lowest loss on validation set, or doing k-fold\n",
    "cross validation.\n",
    "\n",
    "Setting $\\lambda$ to be very low means we don’t penalize the\n",
    "complex model much. Setting it to $0$ is the original linear\n",
    "regression. Setting it high means we strongly prefer simpler model, at\n",
    "the cost of how well it fits the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed-form solution of Ridge\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "It’s not hard to find a closed-form solution for Ridge, first write the\n",
    "loss function in matrix notation:\n",
    "\n",
    "\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2 + \\lambda{\\left\\lVert w \\right\\rVert}_2^2\\end{align}\n",
    "\n",
    "Then the gradient is:\n",
    "\n",
    "\\begin{align}\\nabla L_w = -2X^T(y-Xw) + 2\\lambda w\\end{align}\n",
    "\n",
    "Setting to zero and solve:\n",
    "\n",
    "\\begin{align}\\begin{align}\n",
    "   0 &= -2X^T(y-Xw) + 2\\lambda w \\\\\n",
    "   &= X^T(y-Xw) - \\lambda w    \\\\ \n",
    "   &= X^Ty - X^TXw - \\lambda w \\\\\n",
    "   &= X^Ty - (X^TX + \\lambda I_d) w\n",
    "   \\end{align}\\end{align}\n",
    "\n",
    "Move that to other side and we get a closed-form solution:\n",
    "\n",
    "\\begin{align}\\begin{align}\n",
    "   (X^TX + \\lambda I_d) w &= X^Ty    \\\\\n",
    "   w &= (X^TX + \\lambda I_d)^{-1}X^Ty\n",
    "   \\end{align}\\end{align}\n",
    "\n",
    "which is almost the same as linear regression without regularization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. L1 Penalty (or Lasso)\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "As you might guess, you can also use L1-norm for **L1 regularization**:\n",
    "\n",
    "\\begin{align}L(w) = \\sum_{i=1}^{n} \\left( y^i - wx^i \\right)^2 + \\lambda\\sum_{j=0}^{d}\\left|w_j\\right|\\end{align}\n",
    "\n",
    "Again, in fancy term, this loss function is also known as **Lasso\n",
    "regression**. Using matrix notation:\n",
    "\n",
    "\\begin{align}L(w) = {\\left\\lVert y - Xw \\right\\rVert}^2 + \\lambda{\\left\\lVert w \\right\\rVert}_1\\end{align}\n",
    "\n",
    "It’s more complex to get a closed-form solution for this, so we’ll leave\n",
    "it here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Loss Surface with Regularization\n",
    "------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see what these penalty terms mean geometrically.\n",
    "\n",
    "L2 loss surface\n",
    "~~~~~~~~~~~~~~~\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |l2 surface|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "This simply follows the 3D equation:\n",
    "\n",
    "\\begin{align}L(w) = {\\left\\lVert w \\right\\rVert}_2^2 = w_0^2 + w_1^2\\end{align}\n",
    "\n",
    "The center of the bowl is lowest, since ``w = [0,0]``, but that is not\n",
    "even a line and it won’t predict anything useful.\n",
    "\n",
    "L2 loss surface under different lambdas\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "When you multiply the L2 norm function with lambda,\n",
    "$L(w) = \\lambda(w_0^2 + w_1^2)$, the width of the bowl changes.\n",
    "The lowest (and flattest) one has lambda of 0.25, which you can see it\n",
    "penalizes The two subsequent ones has lambdas of 0.5 and 1.0.\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |l2 surface many lambdas|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "L1 loss surface\n",
    "~~~~~~~~~~~~~~~\n",
    "\n",
    "Below is the loss surface of L1 penalty:\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |l1 surface|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "Similarly the equation is\n",
    "$L(w) = \\lambda(\\left| w_0 \\right| + \\left| w_1 \\right|)$.\n",
    "\n",
    "Contour of different penalty terms\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "If the L2 norm is 1, you get a unit circle ($w_0^2 + w_1^2 = 1$).\n",
    "In the same manner, you get “unit” shapes in other norms:\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |norm contours|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "**When you walk along these lines, you get the same loss, which is 1**\n",
    "\n",
    "These shapes can hint us different behaviors of each norm, which brings\n",
    "us to the next question.\n",
    "\n",
    ".. |l2 surface| image:: imgs/img_l2_surface.png\n",
    ".. |l2 surface many lambdas| image:: imgs/img_l2_surface_lambdas.png\n",
    ".. |l1 surface| image:: imgs/img_l1_surface.png\n",
    ".. |norm contours| image:: imgs/img_penalty_contours.png\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one to use, L1 or L2?\n",
    "---------------------------\n",
    "\n",
    "What’s the point of using different penalty terms, as it seems like both\n",
    "try to push down the size of $w$.\n",
    "\n",
    "**Turns out L1 penalty tends to produce sparse solutions**. This means\n",
    "many entries in $w$ are zeros. This is good if you want the model\n",
    "to be simple and compact. Why is that?\n",
    "\n",
    "Geometrical Explanation\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "*Note: these figures are generated with unusually high lambda to\n",
    "exaggerate the plot*\n",
    "\n",
    "First let’s bring both linear regression and penalty loss surface\n",
    "together (left), and recall that we want to find the **minimum loss when\n",
    "both surfaces are summed up** (right):\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |ridge regression|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "Ridge regression is like finding the middle point where the loss of a\n",
    "sum between linear regression and L2 penalty loss is lowest:\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |ridge regression sol 30|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "You can imagine starting with the linear regression solution (red point)\n",
    "where the loss is the lowest, then you move towards the origin (blue\n",
    "point), where the penalty loss is lowest. **The more lambda you set, the\n",
    "more you’ll be drawn towards the origin, since you penalize the values\n",
    "of $w_i$ more** so it wants to get to where they’re all zeros:\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |ridge regression sol 60|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "Since the loss surfaces of linear regression and L2 norm are both\n",
    "ellipsoid, the solution found for Ridge regression **tends to be\n",
    "directly between both solutions**. Notice how the summed ellipsoid is\n",
    "still right in the middle.\n",
    "\n",
    "--------------\n",
    "\n",
    "For Lasso:\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |lasso regression|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "And this is the Lasso solution for lambda = 30 and 60:\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |lasso regression sol 30|\n",
    "\n",
    "| |lasso regression sol 60|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "Notice that the ellipsoid of linear regression **approaches, and finally\n",
    "hits a corner of L1 loss**, and will always stay at that corner. What\n",
    "does a corner of L1 norm means in this situation? It means\n",
    "$w_1 = 0$.\n",
    "\n",
    "Again, this is because the contour lines **at the same loss value** of\n",
    "L2 norm reaches out much farther than L1 norm:\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   <center>\n",
    "\n",
    "| |l1 vs l2|\n",
    "\n",
    ".. raw:: html\n",
    "\n",
    "   </center>\n",
    "\n",
    "If the linear regression finds an optimal contact point along the L2\n",
    "circle, then it will stop since there’s no use to move sideways where\n",
    "the loss is usually higher. However, with L1 penalty, it can drift\n",
    "toward a corner, because it’s **the same loss along the line** anyway (I\n",
    "mean, why not?) and thus is exploited, if the opportunity arises.\n",
    "\n",
    ".. |ridge regression| image:: imgs/img_ridge_regression.png\n",
    ".. |ridge regression sol 30| image:: imgs/img_ridge_sol_30.png\n",
    ".. |ridge regression sol 60| image:: imgs/img_ridge_sol_60.png\n",
    ".. |lasso regression| image:: imgs/img_lasso_regression.png\n",
    ".. |lasso regression sol 30| image:: imgs/img_lasso_sol_30.png\n",
    ".. |lasso regression sol 60| image:: imgs/img_lasso_sol_60.png\n",
    ".. |l1 vs l2| image:: imgs/img_l1_vs_l2_contour.png\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
